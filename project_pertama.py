# -*- coding: utf-8 -*-
"""project_pertama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A7iLC-U-IJqN6gJPZSo-4aybaQwTQGJn

# 1. import library
disini penulis menuliskan library yang diperlukan untuk membangun model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import kagglehub
from kagglehub import KaggleDatasetAdapter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

"""# 2. Load dataset
Dataset yang digunakan dalam proyek ini diambil dari Kaggle: Student Depression Dataset https://www.kaggle.com/datasets/hopesb/student-depression-dataset, yang berisi informasi tentang kondisi mental mahasiswa dan faktor-faktor yang dapat berhubungan dengan depresi.
"""

# Set the path to the file you'd like to load
file_path = "student_depression_dataset.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "adilshamim8/student-depression-dataset",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

print("First 5 records:", df.head())

"""# 3. Exploratory Data Analysis (EDA)
Exploratory Data Analysis (EDA) adalah proses awal yang sangat penting dalam analisis data yang bertujuan untuk memahami struktur dan pola dalam data sebelum melakukan analisis lebih lanjut. Langkah pertama dalam EDA adalah memahami tujuan analisis dan melihat data secara keseluruhan, termasuk memeriksa ukuran, jenis data, dan nilai yang hilang.
"""

df.describe()

df.info()

df.isnull().sum()

df.duplicated().sum()

df.Profession.unique()

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x='Age')
plt.title('Boxplot Umur Responden')
plt.xlabel('Usia (Tahun)')
plt.show()

df.describe()

"""# 4. Data Preparation
Data Preparation adalah langkah krusial dalam proses analisis data yang bertujuan untuk memastikan data siap digunakan dalam analisis atau pemodelan. Langkah pertama adalah pembersihan data, yang mencakup penanganan nilai yang hilang, duplikasi, dan outliers untuk memastikan kualitas data. Selanjutnya, transformasi data dilakukan, seperti normalisasi atau standarisasi untuk menyamakan skala data, serta pengkodean variabel kategorikal menjadi format numerik agar dapat digunakan dalam analisis.
"""

def remove_outliers_iqr(df, column_name):
    Q1 = df[column_name].quantile(0.25)
    Q3 = df[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter data tanpa outlier
    df_filtered = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]
    return df_filtered

# untuk kolom 'Age'
df = remove_outliers_iqr(df, 'Age')

# Langkah 1: Membersihkan format Profesi (hilangkan tanda kutip)
df['Profession'] = df['Profession'].str.replace("'", "")

# Langkah 2: Menghitung jumlah data sebelum dihapus
print(f"Jumlah data sebelum filter: {len(df)}")

# Langkah 3: Menyimpan hanya baris dengan Profesi 'Student'
df.drop(df[df['Profession'] != 'Student'].index, inplace=True)

# Langkah 4: Reset index karena banyak baris terhapus
df.reset_index(drop=True, inplace=True)

# Langkah 5: Verifikasi hasil
print(f"Jumlah data setelah filter: {len(df)}")
df.Profession.unique()

# Ambil fitur numerik yang relevan
selected_features = [
    'Age', 'Academic Pressure', 'Work Pressure',
    'CGPA', 'Study Satisfaction', 'Job Satisfaction',
    'Work/Study Hours', 'Depression'
]

# Hitung korelasi hanya antar fitur tersebut
corr = df[selected_features].corr()

# Plot heatmap-nya
plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Heatmap Korelasi (Fitur Terkait Depresi)")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

sns.catplot(
    x='Work/Study Hours',
    hue='Depression',
    col='Gender',
    data=df,
    kind='count',
    aspect=1.5,   # lebar rasio antar kolom
    height=5      # tinggi plot (optional, bisa diubah juga)
)

sns.countplot(x='Family History of Mental Illness', hue='Depression', data=df)
plt.title('Pengaruh Riwayat Penyakit Mental Keluarga terhadap Depresi')

count = df.groupby(['Family History of Mental Illness', 'Depression']).size().reset_index(name='Jumlah')
print(count)

"""# 5. Membangun Model
**Membangun model** dalam analisis data adalah proses yang melibatkan pemilihan algoritma yang sesuai untuk menyelesaikan masalah tertentu, seperti prediksi atau klasifikasi. Langkah pertama adalah memilih algoritma yang tepat, tergantung pada jenis data dan tujuan analisis. Misalnya, untuk masalah klasifikasi, model seperti Logistic Regression atau Decision Tree dapat digunakan, sementara untuk prediksi numerik, algoritma regresi seperti Linear Regression atau Random Forest Regression lebih cocok. Setelah algoritma dipilih, data yang telah dibersihkan dan diformat dengan benar dibagi menjadi data pelatihan dan data pengujian. Data pelatihan digunakan untuk melatih model, sedangkan data pengujian digunakan untuk mengukur kinerja model setelah proses pelatihan. Proses pelatihan melibatkan penyesuaian parameter model agar dapat memprediksi atau mengklasifikasikan data dengan akurat. Setelah model dilatih, evaluasi dilakukan dengan menggunakan metrik seperti akurasi, precision, recall, atau RMSE untuk menentukan sejauh mana model dapat diandalkan dalam membuat prediksi pada data baru.

"""

## 1. Persiapan Data
# Pilih fitur berdasarkan analisis korelasi
selected_features = ['Academic Pressure', 'Age', 'Work/Study Hours', 'Study Satisfaction']
target = 'Depression'

# Create new features
df['Pressure_Hours_Ratio'] = df['Academic Pressure'] / (df['Work/Study Hours'] + 1)
df['Age_Study_Satisfaction'] = df['Age'] * df['Study Satisfaction']

# Updated feature list
enhanced_features = selected_features + ['Pressure_Hours_Ratio', 'Age_Study_Satisfaction']

# Membuat dataset baru dengan fitur terpilih
X = df[enhanced_features]
y = df[target]

# Split data training-testing (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardisasi fitur
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

## 3. Membangun Model
# Inisialisasi model Random Forest
best_params = {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}
# Unpack the dictionary using ** to pass parameters correctly
model = RandomForestClassifier(**best_params, random_state=42, class_weight='balanced')

# Training model
model.fit(X_train_scaled, y_train)

## 4. Evaluasi Model
# Prediksi pada data test
y_pred = model.predict(X_test_scaled)

# Hasil evaluasi
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# 6. Hyperparameter tuning
Hyperparameter tuning adalah proses untuk menemukan nilai terbaik dari hyperparameter yang digunakan dalam model machine learning agar model dapat memberikan hasil yang optimal. Hyperparameter adalah parameter yang ditentukan sebelum proses pelatihan model dan mempengaruhi cara model belajar. Contoh hyperparameter termasuk jumlah pohon dalam Random Forest, kedalaman maksimum pohon dalam Decision Tree, atau nilai learning rate dalam Gradient Descent.
"""

## 5. Analisis Feature Importance
# Mendapatkan importance score
feature_importance = pd.DataFrame({
    'Feature': enhanced_features,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

# Visualisasi feature importance
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis')
plt.title('Feature Importance untuk Prediksi Depresi')
plt.show()

## 6. Optimasi Model (Opsional)
from sklearn.model_selection import GridSearchCV

# Tentukan parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Grid search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')
grid_search.fit(X_train_scaled, y_train)

# Model terbaik
best_model = grid_search.best_estimator_
print("\nBest Parameters:", grid_search.best_params_)

# 3. Train Model
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train_scaled, y_train)

# 4. Evaluate
y_pred = model.predict(X_test_scaled)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(classification_report(y_test, y_pred))

# 5. Coefficients - PERBAIKAN DI SINI
print("\nFeature Coefficients:")
for feat, coef in zip(enhanced_features, model.coef_[0]):  # Gunakan enhanced_features
    print(f"{feat}: {coef:.4f}")

# 3. Train Model
tree_model = DecisionTreeClassifier(random_state=42, class_weight='balanced')  # pakai class_weight jika imbalance
tree_model.fit(X_train_scaled, y_train)

# 4. Evaluate
y_pred_tree = tree_model.predict(X_test_scaled)
print(f"Accuracy: {accuracy_score(y_test, y_pred_tree):.2f}")
print(classification_report(y_test, y_pred_tree))

# 5. Visualisasi confusion matrix
ConfusionMatrixDisplay.from_estimator(tree_model, X_test_scaled, y_test)
plt.title("Decision Tree Confusion Matrix")
plt.show()

# 6. Feature Importances
print("\nFeature Importances:")
for feat, imp in zip(enhanced_features, tree_model.feature_importances_):
    print(f"{feat}: {imp:.4f}")

""" # 7. model testing
 Mencoba mentesting model dengan data baru agar bisa mengevaluasi model
"""

# Contoh data baru untuk prediksi
new_data = pd.DataFrame({
    'Academic Pressure': [3.5],
    'Age': [22],
    'Work/Study Hours': [8],
    'Study Satisfaction': [2.5]
})

# Create new features for the new data as well:
new_data['Pressure_Hours_Ratio'] = new_data['Academic Pressure'] / (new_data['Work/Study Hours'] + 1)
new_data['Age_Study_Satisfaction'] = new_data['Age'] * new_data['Study Satisfaction']


# Preprocess data baru
new_data_scaled = scaler.transform(new_data)

# Prediksi
prediction = best_model.predict(new_data_scaled)
print("\nPrediksi Depresi untuk Data Baru:", prediction)